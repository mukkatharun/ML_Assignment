{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Assignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNS+LLJKcu50wtDA4n0TXGS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukkatharun/ML_Assignment/blob/main/ML_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definitions, Examples of Key Concepts**\n",
        "\n",
        "Select your Top 5 concepts, formulas, definitions, etc that you have studied in the greatest detail, explain why they are important.\n",
        "\n",
        "Write the definitions and give an example for each"
      ],
      "metadata": {
        "id": "X5EnayvlzvMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Clustering Techniques**\n",
        "\n",
        "**Defination** : Cluster analysis, or clustering, is an unsupervised machine learning task. It involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.\n",
        "\n",
        "**K-Means Clustering**:\n",
        "K-Means attempts to classify data without having first been trained with labeled data.\n",
        "\n",
        "**Examples**:\n",
        "We can use the clustering techniques in \"Identifying Fake News\" and \"Spam filter\" use cases. "
      ],
      "metadata": {
        "id": "TVitUFG874y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. TF-IDF**\n",
        "\n",
        "**Definition**: In information retrieval, tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
        "\n",
        "This concept is really helpful when you are working with text documents and want to identify the important words in each document when there are so many non-related words\n",
        "\n",
        "**TF (term frequency)** :  Number of times the word appears in a document (raw count)\n",
        "tf(term) = count of the word occurered in document\n",
        "\n",
        "**IDF (inverse document frequency)** : Inverse document frequency looks at how common (or uncommon) a word is amongst the corpus. IDF is calculated as follows where t is the term (word) we are looking to measure the commonness of and N is the number of documents (d) in the corpus (D).. The denominator is simply the number of documents in which the term, t, \n",
        "\n",
        "idf(term) = ln(N_documents/N_documents containing term)\n",
        "\n",
        "**Example**:\n",
        " It's a scoring measure widely used in information retrieval (IR) or summarization "
      ],
      "metadata": {
        "id": "N_6iVBJA2Hsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Logistic Regression**\n",
        "\n",
        "**Definition**: The Logistic model is used to model the probability of a certain class or event existing such as pass/fail, win/lose, alive/dead or healthy/sick.\n",
        "\n",
        "It is a simple alogrithm but very efficient method for binary and linear classification problems.\n",
        "\n",
        "Examples: credit card fraudulent transaction detection"
      ],
      "metadata": {
        "id": "jFfmCKRq_SNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Principal component analysis (PCA)**\n",
        "\n",
        "**Definition**: it is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.\n",
        "\n",
        "When we have so many dimeansions, we can use techniques like PCA to reduce the dimensions which will help in effcitive model training time and evalution\n",
        "\n",
        "**Examples**:  when we high dimensional data, we can reduce them into lower dimensional data which are PCA transformed columns, where first 3-4 columns gives us 95%+ accurate representation of original data."
      ],
      "metadata": {
        "id": "izlaqTNzMNdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Missing Values Handling**\n",
        "Having inconsistent data is always a drawback while model building, So the better we handle the missing values, the better it will be while building the models. As the saying goes \" Junk in Junk out\"\n",
        "\n",
        "There are so many ways to fill the missing values\n",
        "\n",
        "\n",
        "1.   If the missing values are really less % when compared to the original data, then we can go ahead and delete those particular records\n",
        "2.   If we have signification amount of missing value and if that particular feature is important in model building, then we can explore option like filling the values with Mean, Median, values generated using logistic regression or some prediction model algorithm\n",
        "\n",
        "**Examples**: if we have age column data is missing, we can either choose the mean or median values to fill the values or use some prediction algorithm based on other features when we have significant amount of missing values"
      ],
      "metadata": {
        "id": "_6ODbtjnUJC-"
      }
    }
  ]
}